{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611ca0dc-8ad5-450a-b9a7-748c7eddc885",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "611ca0dc-8ad5-450a-b9a7-748c7eddc885",
        "outputId": "2856c091-0e26-4e4c-ce86-ac22890d8775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=60256faff603135feb832417d281b317e4a92361e617881299805c006b412a80\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Collecting sparkmeasure\n",
            "  Downloading sparkmeasure-0.24.0-py2.py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: sparkmeasure\n",
            "Successfully installed sparkmeasure-0.24.0\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "since the dataset is from kaggle we can download it with opendataset but we need an access token which is expire after a while\n",
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/pigment/big-sales-data\")\n",
        "'''\n",
        "!wget https://users.itk.ppke.hu/~pasda2/Books_rating.csv.zip\n",
        "!unzip Books_rating.csv.zip\n",
        "\n",
        "!pip install pyspark\n",
        "!pip install sparkmeasure\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22db6cd5-1d1d-4053-8f54-e19a21cd3254",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "22db6cd5-1d1d-4053-8f54-e19a21cd3254",
        "outputId": "3b2f031d-59c6-497c-9d0b-2e55da4f350a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54.401153802871704\n",
            "22\n",
            "['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "root\n",
            " |-- Id: float (nullable = true)\n",
            " |-- Title: float (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- User_id: float (nullable = true)\n",
            " |-- profileName: float (nullable = true)\n",
            " |-- review/helpfulness: float (nullable = true)\n",
            " |-- review/score: float (nullable = true)\n",
            " |-- review/time: float (nullable = true)\n",
            " |-- review/summary: float (nullable = true)\n",
            " |-- review/text: float (nullable = true)\n",
            "\n",
            "1.1308960914611816\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "|         Id|Title|Price|User_id|profileName|review/helpfulness|review/score|review/time|review/summary|review/text|\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "|1.8829312E9|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0| 9.406368E8|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.0957248E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.0787904E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.0907136E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.1079936E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.1271744E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.1001312E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|   1.2312E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.2098592E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.0763712E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|   9.9144E8|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|1.2917664E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|1.2483072E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|  1.22256E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         1.0|1.1170656E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         4.0|1.1195712E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         1.0|1.1192256E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1159424E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1170656E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1300256E9|           0.0|        0.0|\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "5.856187343597412\n",
            "['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "root\n",
            " |-- Id: float (nullable = false)\n",
            " |-- Title: float (nullable = false)\n",
            " |-- Price: float (nullable = false)\n",
            " |-- User_id: float (nullable = false)\n",
            " |-- profileName: float (nullable = false)\n",
            " |-- review/helpfulness: float (nullable = false)\n",
            " |-- review/score: float (nullable = false)\n",
            " |-- review/time: float (nullable = false)\n",
            " |-- review/summary: float (nullable = false)\n",
            " |-- review/text: float (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            "\n",
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- score: float (nullable = false)\n",
            "\n",
            "1.6689300537109375e-06\n",
            "2.1457672119140625e-06\n",
            "1.6689300537109375e-06\n",
            "R Squared (R2) on test data = 8.95524e-05\n",
            "Root Mean Squared Error (RMSE) on test data = 1.56095e+06\n",
            "458.2271852493286\n",
            "RMSE: 1386627.099366\n",
            "r2: 0.000071\n",
            "numIterations: 7\n",
            "objectiveHistory: [0.4999999999999999, 0.4999759035188339, 0.4999693576703217, 0.49996460585778185, 0.49996458220237644, 0.4999645775781211, 0.4999645775621629, 0.49996457756198665]\n",
            "[{'algorithm': 'linear_regression', 'read_csv': 54.40115165710449, 'number_of_partitions': 22, 'data_convertion_time': 7.041205167770386, 'data_scale_time': 167.69301748275757, 'data_split_time': 0.09982728958129883, 'model_fit_time': 476.8695456981659, 'prediction_time': 0.17137622833251953, 'evaulation_time': 458.2271785736084, 'full_time': 1164.7111463546753}]\n",
            "40.4030966758728\n",
            "22\n",
            "['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "root\n",
            " |-- Id: float (nullable = true)\n",
            " |-- Title: float (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- User_id: float (nullable = true)\n",
            " |-- profileName: float (nullable = true)\n",
            " |-- review/helpfulness: float (nullable = true)\n",
            " |-- review/score: float (nullable = true)\n",
            " |-- review/time: float (nullable = true)\n",
            " |-- review/summary: float (nullable = true)\n",
            " |-- review/text: float (nullable = true)\n",
            "\n",
            "0.46886539459228516\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "|         Id|Title|Price|User_id|profileName|review/helpfulness|review/score|review/time|review/summary|review/text|\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "|1.8829312E9|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0| 9.406368E8|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.0957248E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.0787904E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.0907136E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.1079936E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.1271744E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.1001312E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|   1.2312E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.2098592E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.0763712E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|   9.9144E8|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|1.2917664E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|1.2483072E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|  1.22256E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         1.0|1.1170656E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         4.0|1.1195712E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         1.0|1.1192256E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1159424E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1170656E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1300256E9|           0.0|        0.0|\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "1.3680531978607178\n",
            "['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "root\n",
            " |-- Id: float (nullable = false)\n",
            " |-- Title: float (nullable = false)\n",
            " |-- Price: float (nullable = false)\n",
            " |-- User_id: float (nullable = false)\n",
            " |-- profileName: float (nullable = false)\n",
            " |-- review/helpfulness: float (nullable = false)\n",
            " |-- review/score: float (nullable = false)\n",
            " |-- review/time: float (nullable = false)\n",
            " |-- review/summary: float (nullable = false)\n",
            " |-- review/text: float (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            "\n",
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- score: float (nullable = false)\n",
            "\n",
            "1.9073486328125e-06\n",
            "1.430511474609375e-06\n",
            "7.152557373046875e-07\n",
            "1.1920928955078125e-06\n",
            "R Squared (R2) on test data = 0.000592183\n",
            "Root Mean Squared Error (RMSE) on test data = 1.56056e+06\n",
            "441.7397699356079\n",
            "[{'algorithm': 'linear_regression', 'read_csv': 54.40115165710449, 'number_of_partitions': 22, 'data_convertion_time': 7.041205167770386, 'data_scale_time': 167.69301748275757, 'data_split_time': 0.09982728958129883, 'model_fit_time': 476.8695456981659, 'prediction_time': 0.17137622833251953, 'evaulation_time': 458.2271785736084, 'full_time': 1164.7111463546753}, {'algorithm': 'decision_tree_regression', 'read_csv': 40.40309381484985, 'number_of_partitions': 22, 'data_convertion_time': 1.6531219482421875, 'data_scale_time': 146.76617550849915, 'data_split_time': 0.04237866401672363, 'model_fit_time': 611.9974579811096, 'prediction_time': 0.30161142349243164, 'evaulation_time': 441.73979902267456, 'full_time': 1243.0479381084442}]\n",
            "['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "root\n",
            " |-- Id: float (nullable = true)\n",
            " |-- Title: float (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- User_id: float (nullable = true)\n",
            " |-- profileName: float (nullable = true)\n",
            " |-- review/helpfulness: float (nullable = true)\n",
            " |-- review/score: float (nullable = true)\n",
            " |-- review/time: float (nullable = true)\n",
            " |-- review/summary: float (nullable = true)\n",
            " |-- review/text: float (nullable = true)\n",
            "\n",
            "0.3284730911254883\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "|         Id|Title|Price|User_id|profileName|review/helpfulness|review/score|review/time|review/summary|review/text|\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "|1.8829312E9|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0| 9.406368E8|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.0957248E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.0787904E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.0907136E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.1079936E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.1271744E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.1001312E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|   1.2312E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.2098592E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.0763712E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|   9.9144E8|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|1.2917664E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|1.2483072E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|  1.22256E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         1.0|1.1170656E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         4.0|1.1195712E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         1.0|1.1192256E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1159424E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1170656E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1300256E9|           0.0|        0.0|\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "1.0464346408843994\n",
            "['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "root\n",
            " |-- Id: float (nullable = false)\n",
            " |-- Title: float (nullable = false)\n",
            " |-- Price: float (nullable = false)\n",
            " |-- User_id: float (nullable = false)\n",
            " |-- profileName: float (nullable = false)\n",
            " |-- review/helpfulness: float (nullable = false)\n",
            " |-- review/score: float (nullable = false)\n",
            " |-- review/time: float (nullable = false)\n",
            " |-- review/summary: float (nullable = false)\n",
            " |-- review/text: float (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            "\n",
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- score: float (nullable = false)\n",
            "\n",
            "1.430511474609375e-06\n",
            "1.1920928955078125e-06\n",
            "R Squared (R2) on test data = 8.24975e-05\n",
            "Root Mean Squared Error (RMSE) on test data = 1.56096e+06\n",
            "[{'algorithm': 'linear_regression', 'read_csv': 54.40115165710449, 'number_of_partitions': 22, 'data_convertion_time': 7.041205167770386, 'data_scale_time': 167.69301748275757, 'data_split_time': 0.09982728958129883, 'model_fit_time': 476.8695456981659, 'prediction_time': 0.17137622833251953, 'evaulation_time': 458.2271785736084, 'full_time': 1164.7111463546753}, {'algorithm': 'decision_tree_regression', 'read_csv': 40.40309381484985, 'number_of_partitions': 22, 'data_convertion_time': 1.6531219482421875, 'data_scale_time': 146.76617550849915, 'data_split_time': 0.04237866401672363, 'model_fit_time': 611.9974579811096, 'prediction_time': 0.30161142349243164, 'evaulation_time': 441.73979902267456, 'full_time': 1243.0479381084442}, {'algorithm': 'random_forest_regression', 'read_csv': 35.80125308036804, 'number_of_partitions': 22, 'data_convertion_time': 1.2152087688446045, 'data_scale_time': 145.57173871994019, 'data_split_time': 0.01819586753845215, 'model_fit_time': 673.2231960296631, 'prediction_time': 0.9374315738677979, 'evaulation_time': 456.506267786026, 'full_time': 1313.3309388160706}]\n",
            "456.5064117908478\n",
            "['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "root\n",
            " |-- Id: float (nullable = true)\n",
            " |-- Title: float (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- User_id: float (nullable = true)\n",
            " |-- profileName: float (nullable = true)\n",
            " |-- review/helpfulness: float (nullable = true)\n",
            " |-- review/score: float (nullable = true)\n",
            " |-- review/time: float (nullable = true)\n",
            " |-- review/summary: float (nullable = true)\n",
            " |-- review/text: float (nullable = true)\n",
            "\n",
            "0.39742445945739746\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "|         Id|Title|Price|User_id|profileName|review/helpfulness|review/score|review/time|review/summary|review/text|\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "|1.8829312E9|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0| 9.406368E8|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.0957248E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.0787904E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.0907136E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.1079936E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.1271744E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.1001312E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|   1.2312E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.2098592E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.0763712E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|   9.9144E8|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|1.2917664E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|1.2483072E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|  1.22256E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         1.0|1.1170656E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         4.0|1.1195712E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         1.0|1.1192256E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1159424E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1170656E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1300256E9|           0.0|        0.0|\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "1.1871426105499268\n",
            "['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "root\n",
            " |-- Id: float (nullable = false)\n",
            " |-- Title: float (nullable = false)\n",
            " |-- Price: float (nullable = false)\n",
            " |-- User_id: float (nullable = false)\n",
            " |-- profileName: float (nullable = false)\n",
            " |-- review/helpfulness: float (nullable = false)\n",
            " |-- review/score: float (nullable = false)\n",
            " |-- review/time: float (nullable = false)\n",
            " |-- review/summary: float (nullable = false)\n",
            " |-- review/text: float (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            "\n",
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- score: float (nullable = false)\n",
            "\n",
            "1.6689300537109375e-06\n",
            "0.02199864387512207\n",
            "1.1920928955078125e-06\n",
            "R Squared (R2) on test data = 0.0563759\n",
            "Root Mean Squared Error (RMSE) on test data = 1.51638e+06\n",
            "427.6495373249054\n",
            "[{'algorithm': 'linear_regression', 'read_csv': 54.40115165710449, 'number_of_partitions': 22, 'data_convertion_time': 7.041205167770386, 'data_scale_time': 167.69301748275757, 'data_split_time': 0.09982728958129883, 'model_fit_time': 476.8695456981659, 'prediction_time': 0.17137622833251953, 'evaulation_time': 458.2271785736084, 'full_time': 1164.7111463546753}, {'algorithm': 'decision_tree_regression', 'read_csv': 40.40309381484985, 'number_of_partitions': 22, 'data_convertion_time': 1.6531219482421875, 'data_scale_time': 146.76617550849915, 'data_split_time': 0.04237866401672363, 'model_fit_time': 611.9974579811096, 'prediction_time': 0.30161142349243164, 'evaulation_time': 441.73979902267456, 'full_time': 1243.0479381084442}, {'algorithm': 'random_forest_regression', 'read_csv': 35.80125308036804, 'number_of_partitions': 22, 'data_convertion_time': 1.2152087688446045, 'data_scale_time': 145.57173871994019, 'data_split_time': 0.01819586753845215, 'model_fit_time': 673.2231960296631, 'prediction_time': 0.9374315738677979, 'evaulation_time': 456.506267786026, 'full_time': 1313.3309388160706}, {'algorithm': 'gradient_boosted_regression', 'read_csv': 35.14917469024658, 'number_of_partitions': 22, 'data_convertion_time': 1.4740698337554932, 'data_scale_time': 152.96780633926392, 'data_split_time': 0.022151947021484375, 'model_fit_time': 1157.391065120697, 'prediction_time': 1.025705099105835, 'evaulation_time': 427.64956974983215, 'full_time': 1775.7636816501617}]\n",
            "427.64972281455994\n",
            "['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "root\n",
            " |-- Id: float (nullable = true)\n",
            " |-- Title: float (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- User_id: float (nullable = true)\n",
            " |-- profileName: float (nullable = true)\n",
            " |-- review/helpfulness: float (nullable = true)\n",
            " |-- review/score: float (nullable = true)\n",
            " |-- review/time: float (nullable = true)\n",
            " |-- review/summary: float (nullable = true)\n",
            " |-- review/text: float (nullable = true)\n",
            "\n",
            "0.20524263381958008\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "|         Id|Title|Price|User_id|profileName|review/helpfulness|review/score|review/time|review/summary|review/text|\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "|1.8829312E9|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0| 9.406368E8|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.0957248E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.0787904E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.0907136E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.1079936E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.1271744E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.1001312E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|   1.2312E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         5.0|1.2098592E9|           0.0|        0.0|\n",
            "|8.2641434E8|  0.0|  0.0|    0.0|        0.0|               0.0|         4.0|1.0763712E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|   9.9144E8|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|1.2917664E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|1.2483072E9|           0.0|        0.0|\n",
            "|8.2981402E8|  0.0| 19.4|    0.0|        0.0|               0.0|         5.0|  1.22256E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         1.0|1.1170656E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         4.0|1.1195712E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         1.0|1.1192256E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1159424E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1170656E9|           0.0|        0.0|\n",
            "|5.9534458E8|  0.0|10.95|    0.0|        0.0|               0.0|         5.0|1.1300256E9|           0.0|        0.0|\n",
            "+-----------+-----+-----+-------+-----------+------------------+------------+-----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "0.6441493034362793\n",
            "['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "root\n",
            " |-- Id: float (nullable = false)\n",
            " |-- Title: float (nullable = false)\n",
            " |-- Price: float (nullable = false)\n",
            " |-- User_id: float (nullable = false)\n",
            " |-- profileName: float (nullable = false)\n",
            " |-- review/helpfulness: float (nullable = false)\n",
            " |-- review/score: float (nullable = false)\n",
            " |-- review/time: float (nullable = false)\n",
            " |-- review/summary: float (nullable = false)\n",
            " |-- review/text: float (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            "\n",
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- score: float (nullable = false)\n",
            "\n",
            "1.430511474609375e-06\n",
            "1.430511474609375e-06\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o1923.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 252.0 failed 1 times, most recent failure: Lost task 2.0 in stage 252.0 (TID 4839) (1863e1f4e1c7 executor driver): java.lang.RuntimeException: Labels MUST be Integers, but got 19.950000762939453\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_2$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:166)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:84)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.RuntimeException: Labels MUST be Integers, but got 19.950000762939453\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_2$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:166)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:84)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-50c2ef6c7e02>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMulticlassClassificationEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multinomial\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_fit_time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0mst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1923.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 252.0 failed 1 times, most recent failure: Lost task 2.0 in stage 252.0 (TID 4839) (1863e1f4e1c7 executor driver): java.lang.RuntimeException: Labels MUST be Integers, but got 19.950000762939453\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_2$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:166)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:84)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.RuntimeException: Labels MUST be Integers, but got 19.950000762939453\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_2$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:166)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:84)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ],
      "source": [
        "#Linear Regression\n",
        "\n",
        "import csv\n",
        "import time\n",
        "# We can set here how many times we want to run the algorithms\n",
        "for i in range(5):\n",
        "    %reset -f\n",
        "    import csv\n",
        "    import time\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.context import SparkContext\n",
        "    from pyspark import SparkConf\n",
        "    conf = SparkConf().setAppName(\"Regression test\")#\\\n",
        "    #.set('spark.sql.files.maxPartitionBytes','62914560')\n",
        "    #.set('spark.sql.files.maxPartitionBytes','402653184')\n",
        "\n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(conf=conf)\\\n",
        "        .config(\"spark.ui.port\", \"4050\") \\\n",
        "        .getOrCreate()\n",
        "    fulltime=time.time()\n",
        "    logs=[]\n",
        "    log={}\n",
        "    st=time.time()\n",
        "    df=spark.read.csv(\"Books_rating.csv\", header=True, inferSchema=True)\n",
        "    log[\"algorithm\"]=\"linear_regression\"\n",
        "    log[\"read_csv\"]=time.time()-st\n",
        "    print(time.time()-st)\n",
        "    print(df.rdd.getNumPartitions())\n",
        "    log[\"number_of_partitions\"]=df.rdd.getNumPartitions()\n",
        "    st=time.time()\n",
        "    from pyspark.sql.types import *\n",
        "    columns = []\n",
        "\n",
        "    for i in df.dtypes:\n",
        "        columns.append(i[0])\n",
        "    print(columns)\n",
        "\n",
        "    # Write a custom function to convert the data type of DataFrame columns\n",
        "    def convertColumn(df, names, newType):\n",
        "        for name in names:\n",
        "            df = df.withColumn(name, df[name].cast(newType))\n",
        "        return df\n",
        "\n",
        "\n",
        "    # Conver the df columns to FloatType()\n",
        "    df = convertColumn(df, columns, FloatType())\n",
        "\n",
        "    df.fillna(value=0)\n",
        "    df.printSchema()\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from traitlets.traitlets import Float\n",
        "    df=df.fillna(value=0)\n",
        "    df.show()\n",
        "    import numpy as np\n",
        "    from pyspark.sql import functions as F\n",
        "    def convertColumnNeg(df, names):\n",
        "        for name in names:\n",
        "            df = df.withColumn(name,F.when(df[name]<0,0).otherwise(F.col(name)))\n",
        "        return df\n",
        "\n",
        "    df=convertColumnNeg(df,columns)\n",
        "    df=df.replace([np.inf, -np.inf], 0)\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from pyspark.ml.regression import DecisionTreeRegressor\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "    from pyspark.ml.evaluation import RegressionEvaluator\n",
        "    incols=columns\n",
        "    print(incols)\n",
        "    incols.remove(\"review/score\")\n",
        "    assembler = VectorAssembler(inputCols=incols,outputCol=\"features\")\n",
        "    df = assembler.transform(df)\n",
        "    df.printSchema()\n",
        "    final_data = df.select(\"features\", F.col(\"review/score\").alias(\"score\"))\n",
        "    final_data.printSchema()\n",
        "    log[\"data_convertion_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    # Import StandardScaler\n",
        "    from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "    # Initialize the standardScaler\n",
        "    standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "\n",
        "    # Fit the DataFrame to the scaler\n",
        "    scaler = standardScaler.fit(final_data)\n",
        "    # Transform the data in `df` with the scaler\n",
        "    scaled_df = scaler.transform(final_data)\n",
        "    log[\"data_scale_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
        "    log[\"data_split_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    from pyspark.ml.regression import LinearRegression\n",
        "    lr = LinearRegression(featuresCol = 'features', labelCol='score', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "    lr_model = lr.fit(train_data)\n",
        "    log[\"model_fit_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    lr_predictions = lr_model.transform(test_data)\n",
        "    log[\"prediction_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    from pyspark.ml.evaluation import RegressionEvaluator\n",
        "    lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
        "                     labelCol=\"score\",metricName=\"r2\")\n",
        "    print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n",
        "    test_result = lr_model.evaluate(test_data)\n",
        "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)\n",
        "    log[\"evaulation_time\"]=time.time()-st\n",
        "    log[\"full_time\"]=time.time()-fulltime\n",
        "    logs.append(log)\n",
        "    print(time.time()-st)\n",
        "    trainingSummary = lr_model.summary\n",
        "    print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
        "    print(\"r2: %f\" % trainingSummary.r2)\n",
        "    print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
        "    print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
        "    #trainingSummary.residuals.show()\n",
        "    print(logs)\n",
        "\n",
        "    with open(\"logs.csv\", mode='a', newline='') as file:\n",
        "        # Create a CSV writer object\n",
        "        csv_writer = csv.writer(file)\n",
        "\n",
        "        # If the file is empty, write the header\n",
        "        if file.tell() == 0:\n",
        "            csv_writer.writerow(list(log.keys()))\n",
        "        # Write the new data to the end of the CSV file\n",
        "        csv_writer.writerow(list(log.values()))\n",
        "\n",
        "\n",
        "    del df\n",
        "    del train_data\n",
        "    del test_data\n",
        "    del scaled_df\n",
        "    del final_data\n",
        "    del columns\n",
        "    del incols\n",
        "    del log\n",
        "\n",
        "    #Decision Tree Regression\n",
        "\n",
        "    fulltime=time.time()\n",
        "    log={}\n",
        "    st=time.time()\n",
        "    df=spark.read.csv(\"Books_rating.csv\", header=True, inferSchema=True)\n",
        "    log[\"algorithm\"]=\"decision_tree_regression\"\n",
        "    log[\"read_csv\"]=time.time()-st\n",
        "    print(time.time()-st)\n",
        "    print(df.rdd.getNumPartitions())\n",
        "    log[\"number_of_partitions\"]=df.rdd.getNumPartitions()\n",
        "    st=time.time()\n",
        "    from pyspark.sql.types import *\n",
        "    columns = []\n",
        "\n",
        "    for i in df.dtypes:\n",
        "        columns.append(i[0])\n",
        "    print(columns)\n",
        "\n",
        "    # Write a custom function to convert the data type of DataFrame columns\n",
        "    def convertColumn(df, names, newType):\n",
        "        for name in names:\n",
        "            df = df.withColumn(name, df[name].cast(newType))\n",
        "        return df\n",
        "\n",
        "\n",
        "    # Conver the `df` columns to `FloatType()`\n",
        "    df = convertColumn(df, columns, FloatType())\n",
        "\n",
        "    df.fillna(value=0)\n",
        "    df.printSchema()\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from traitlets.traitlets import Float\n",
        "    df=df.fillna(value=0)\n",
        "    df.show()\n",
        "    import numpy as np\n",
        "    from pyspark.sql import functions as F\n",
        "    def convertColumnNeg(df, names):\n",
        "        for name in names:\n",
        "            df = df.withColumn(name,F.when(df[name]<0,0).otherwise(F.col(name)))\n",
        "        return df\n",
        "\n",
        "    df=convertColumnNeg(df,columns)\n",
        "    df=df.replace([np.inf, -np.inf], 0)\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from pyspark.ml.regression import DecisionTreeRegressor\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "    from pyspark.ml.evaluation import RegressionEvaluator\n",
        "    incols=columns\n",
        "    print(incols)\n",
        "    incols.remove(\"review/score\")\n",
        "    assembler = VectorAssembler(inputCols=incols,outputCol=\"features\")\n",
        "    df = assembler.transform(df)\n",
        "    df.printSchema()\n",
        "    final_data = df.select(\"features\", F.col(\"review/score\").alias(\"score\"))\n",
        "    final_data.printSchema()\n",
        "    log[\"data_convertion_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    # Import `StandardScaler`\n",
        "    from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "    # Initialize the `standardScaler`\n",
        "    standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "\n",
        "    # Fit the DataFrame to the scaler\n",
        "    scaler = standardScaler.fit(final_data)\n",
        "    # Transform the data in `df` with the scaler\n",
        "    scaled_df = scaler.transform(final_data)\n",
        "    log[\"data_scale_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
        "    log[\"data_split_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    from pyspark.ml.regression import LinearRegression\n",
        "    dr=DecisionTreeRegressor(featuresCol = 'features', labelCol='score')\n",
        "    model=dr.fit(train_data)\n",
        "    log[\"model_fit_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    dr_predictions =model.transform(test_data)\n",
        "    log[\"prediction_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from pyspark.ml.evaluation import RegressionEvaluator\n",
        "    dr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
        "                     labelCol=\"score\")\n",
        "    print(\"R Squared (R2) on test data = %g\" % dr_evaluator.evaluate(dr_predictions,{dr_evaluator.metricName: \"r2\"}))\n",
        "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % dr_evaluator.evaluate(dr_predictions,{dr_evaluator.metricName: \"rmse\"}))\n",
        "    print(time.time()-st)\n",
        "    log[\"evaulation_time\"]=time.time()-st\n",
        "    log[\"full_time\"]=time.time()-fulltime\n",
        "    logs.append(log)\n",
        "    print(logs)\n",
        "\n",
        "    with open(\"logs.csv\", mode='a', newline='') as file:\n",
        "        # Create a CSV writer object\n",
        "        csv_writer = csv.writer(file)\n",
        "\n",
        "        # If the file is empty, write the header\n",
        "        if file.tell() == 0:\n",
        "            csv_writer.writerow(list(log.keys()))\n",
        "        # Write the new data to the end of the CSV file\n",
        "        csv_writer.writerow(list(log.values()))\n",
        "\n",
        "\n",
        "    del df\n",
        "    del train_data\n",
        "    del test_data\n",
        "    del scaled_df\n",
        "    del final_data\n",
        "    del columns\n",
        "    del incols\n",
        "    del log\n",
        "    del model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Random Forest Regression\n",
        "\n",
        "    fulltime=time.time()\n",
        "    log={}\n",
        "    st=time.time()\n",
        "    df=spark.read.csv(\"Books_rating.csv\", header=True, inferSchema=True)\n",
        "    log[\"algorithm\"]=\"random_forest_regression\"\n",
        "    log[\"read_csv\"]=time.time()-st\n",
        "    log[\"number_of_partitions\"]=df.rdd.getNumPartitions()\n",
        "    st=time.time()\n",
        "\n",
        "    from pyspark.sql.types import *\n",
        "    columns = []\n",
        "\n",
        "    for i in df.dtypes:\n",
        "        columns.append(i[0])\n",
        "    print(columns)\n",
        "\n",
        "    # Write a custom function to convert the data type of DataFrame columns\n",
        "    def convertColumn(df, names, newType):\n",
        "        for name in names:\n",
        "            df = df.withColumn(name, df[name].cast(newType))\n",
        "        return df\n",
        "\n",
        "\n",
        "    # Conver the `df` columns to `FloatType()`\n",
        "    df = convertColumn(df, columns, FloatType())\n",
        "\n",
        "    df.fillna(value=0)\n",
        "    df.printSchema()\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from traitlets.traitlets import Float\n",
        "    df=df.fillna(value=0)\n",
        "    df.show()\n",
        "    import numpy as np\n",
        "    from pyspark.sql import functions as F\n",
        "    def convertColumnNeg(df, names):\n",
        "        for name in names:\n",
        "            df = df.withColumn(name,F.when(df[name]<0,0).otherwise(F.col(name)))\n",
        "        return df\n",
        "\n",
        "    df=convertColumnNeg(df,columns)\n",
        "    df=df.replace([np.inf, -np.inf], 0)\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from pyspark.ml.regression import RandomForestRegressor\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "    from pyspark.ml.evaluation import RegressionEvaluator\n",
        "    incols=columns\n",
        "    print(incols)\n",
        "    incols.remove(\"review/score\")\n",
        "    assembler = VectorAssembler(inputCols=incols,outputCol=\"features\")\n",
        "    df = assembler.transform(df)\n",
        "    df.printSchema()\n",
        "    final_data = df.select(\"features\", F.col(\"review/score\").alias(\"score\"))\n",
        "    final_data.printSchema()\n",
        "    log[\"data_convertion_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    # Import `StandardScaler`\n",
        "    from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "    # Initialize the `standardScaler`\n",
        "    standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "\n",
        "    # Fit the DataFrame to the scaler\n",
        "    scaler = standardScaler.fit(final_data)\n",
        "    # Transform the data in `df` with the scaler\n",
        "    scaled_df = scaler.transform(final_data)\n",
        "    log[\"data_scale_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
        "    log[\"data_split_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "\n",
        "    rf = RandomForestRegressor(featuresCol=\"features\",labelCol=\"score\")\n",
        "    model=rf.fit(train_data)\n",
        "    log[\"model_fit_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    predictions=model.transform(test_data)\n",
        "    log[\"prediction_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    # Select (prediction, true label) and compute test error\n",
        "    evaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\")\n",
        "    print(\"R Squared (R2) on test data = %g\" % evaluator.evaluate(predictions,{evaluator.metricName: \"r2\"}))\n",
        "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % evaluator.evaluate(predictions,{evaluator.metricName: \"rmse\"}))\n",
        "    log[\"evaulation_time\"]=time.time()-st\n",
        "    log[\"full_time\"]=time.time()-fulltime\n",
        "    logs.append(log)\n",
        "    print(logs)\n",
        "    print(time.time()-st)\n",
        "\n",
        "    with open(\"logs.csv\", mode='a', newline='') as file:\n",
        "        # Create a CSV writer object\n",
        "        csv_writer = csv.writer(file)\n",
        "\n",
        "        # If the file is empty, write the header\n",
        "        if file.tell() == 0:\n",
        "            csv_writer.writerow(list(log.keys()))\n",
        "        # Write the new data to the end of the CSV file\n",
        "        csv_writer.writerow(list(log.values()))\n",
        "\n",
        "\n",
        "\n",
        "    del df\n",
        "    del train_data\n",
        "    del test_data\n",
        "    del scaled_df\n",
        "    del final_data\n",
        "    del columns\n",
        "    del incols\n",
        "    del log\n",
        "    del model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Gradient Boosted Regression\n",
        "    fulltime=time.time()\n",
        "    log={}\n",
        "    st=time.time()\n",
        "    df=spark.read.csv(\"Books_rating.csv\", header=True, inferSchema=True)\n",
        "    log[\"algorithm\"]=\"gradient_boosted_regression\"\n",
        "    log[\"read_csv\"]=time.time()-st\n",
        "    log[\"number_of_partitions\"]=df.rdd.getNumPartitions()\n",
        "    st=time.time()\n",
        "\n",
        "    from pyspark.sql.types import *\n",
        "    columns = []\n",
        "\n",
        "    for i in df.dtypes:\n",
        "        columns.append(i[0])\n",
        "    print(columns)\n",
        "\n",
        "    # Write a custom function to convert the data type of DataFrame columns\n",
        "    def convertColumn(df, names, newType):\n",
        "        for name in names:\n",
        "            df = df.withColumn(name, df[name].cast(newType))\n",
        "        return df\n",
        "\n",
        "\n",
        "    # Conver the `df` columns to `FloatType()`\n",
        "    df = convertColumn(df, columns, FloatType())\n",
        "\n",
        "    df.fillna(value=0)\n",
        "    df.printSchema()\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from traitlets.traitlets import Float\n",
        "    df=df.fillna(value=0)\n",
        "    df.show()\n",
        "    import numpy as np\n",
        "    from pyspark.sql import functions as F\n",
        "    def convertColumnNeg(df, names):\n",
        "        for name in names:\n",
        "            df = df.withColumn(name,F.when(df[name]<0,0).otherwise(F.col(name)))\n",
        "        return df\n",
        "\n",
        "    df=convertColumnNeg(df,columns)\n",
        "    df=df.replace([np.inf, -np.inf], 0)\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from pyspark.ml.regression import GBTRegressor\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "    from pyspark.ml.evaluation import RegressionEvaluator\n",
        "    incols=columns\n",
        "    print(incols)\n",
        "    incols.remove(\"review/score\")\n",
        "    assembler = VectorAssembler(inputCols=incols,outputCol=\"features\")\n",
        "    df = assembler.transform(df)\n",
        "    df.printSchema()\n",
        "    final_data = df.select(\"features\", F.col(\"review/score\").alias(\"score\"))\n",
        "    final_data.printSchema()\n",
        "    log[\"data_convertion_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "\n",
        "    # Import `StandardScaler`\n",
        "    from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "    # Initialize the `standardScaler`\n",
        "    standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "\n",
        "    # Fit the DataFrame to the scaler\n",
        "    scaler = standardScaler.fit(final_data)\n",
        "    # Transform the data in `df` with the scaler\n",
        "    scaled_df = scaler.transform(final_data)\n",
        "    log[\"data_scale_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
        "    print(time.time()-st)\n",
        "    log[\"data_split_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "\n",
        "    gbtr = GBTRegressor(featuresCol=\"features\",labelCol=\"score\")\n",
        "    model=gbtr.fit(train_data)\n",
        "    log[\"model_fit_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    predictions=model.transform(test_data)\n",
        "    log[\"prediction_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    # Select (prediction, true label) and compute test error\n",
        "    evaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\")\n",
        "    print(\"R Squared (R2) on test data = %g\" % evaluator.evaluate(predictions,{evaluator.metricName: \"r2\"}))\n",
        "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % evaluator.evaluate(predictions,{evaluator.metricName: \"rmse\"}))\n",
        "    print(time.time()-st)\n",
        "    log[\"evaulation_time\"]=time.time()-st\n",
        "    log[\"full_time\"]=time.time()-fulltime\n",
        "    logs.append(log)\n",
        "    print(logs)\n",
        "    print(time.time()-st)\n",
        "    with open(\"logs.csv\", mode='a', newline='') as file:\n",
        "        # Create a CSV writer object\n",
        "        csv_writer = csv.writer(file)\n",
        "\n",
        "        # If the file is empty, write the header\n",
        "        if file.tell() == 0:\n",
        "            csv_writer.writerow(list(log.keys()))\n",
        "        # Write the new data to the end of the CSV file\n",
        "        csv_writer.writerow(list(log.values()))\n",
        "\n",
        "\n",
        "    del df\n",
        "    del train_data\n",
        "    del test_data\n",
        "    del scaled_df\n",
        "    del final_data\n",
        "    del columns\n",
        "    del incols\n",
        "    del log\n",
        "    del model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #NaiveBayes\n",
        "    fulltime=time.time()\n",
        "    log={}\n",
        "    st=time.time()\n",
        "    df=spark.read.csv(\"Books_rating.csv\", header=True, inferSchema=True)\n",
        "    log[\"algorithm\"]=\"naive_bayes\"\n",
        "    log[\"read_csv\"]=time.time()-st\n",
        "    log[\"number_of_partitions\"]=df.rdd.getNumPartitions()\n",
        "    st=time.time()\n",
        "\n",
        "    from pyspark.sql.types import *\n",
        "    columns = []\n",
        "\n",
        "    for i in df.dtypes:\n",
        "        columns.append(i[0])\n",
        "    print(columns)\n",
        "\n",
        "    # Write a custom function to convert the data type of DataFrame columns\n",
        "    def convertColumn(df, names, newType):\n",
        "        for name in names:\n",
        "            df = df.withColumn(name, df[name].cast(newType))\n",
        "        return df\n",
        "\n",
        "\n",
        "    # Conver the `df` columns to `FloatType()`\n",
        "    df = convertColumn(df, columns, FloatType())\n",
        "\n",
        "    df.fillna(value=0)\n",
        "    df.printSchema()\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from traitlets.traitlets import Float\n",
        "    df=df.fillna(value=0)\n",
        "    df.show()\n",
        "    import numpy as np\n",
        "    from pyspark.sql import functions as F\n",
        "    def convertColumnNeg(df, names):\n",
        "        for name in names:\n",
        "            df = df.withColumn(name,F.when(df[name]<0,0).otherwise(F.col(name)))\n",
        "        return df\n",
        "\n",
        "    df=convertColumnNeg(df,columns)\n",
        "    df=df.replace([np.inf, -np.inf], 0)\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "    from pyspark.ml.evaluation import RegressionEvaluator\n",
        "    incols=columns\n",
        "    print(incols)\n",
        "    incols.remove(\"review/score\")\n",
        "    assembler = VectorAssembler(inputCols=incols,outputCol=\"features\")\n",
        "    df = assembler.transform(df)\n",
        "    df.printSchema()\n",
        "    final_data = df.select(\"features\", F.col(\"review/score\").alias(\"score\"))\n",
        "    final_data.printSchema()\n",
        "    log[\"data_convertion_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    # Import `StandardScaler`\n",
        "    from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "    # Initialize the `standardScaler`\n",
        "    standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "\n",
        "    # Fit the DataFrame to the scaler\n",
        "    scaler = standardScaler.fit(final_data)\n",
        "    # Transform the data in `df` with the scaler\n",
        "    scaled_df = scaler.transform(final_data)\n",
        "    log[\"data_scale_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
        "    log[\"data_split_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "\n",
        "    from pyspark.ml.classification import NaiveBayes\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",labelCol=\"score\")\n",
        "    model = nb.fit(train_data)\n",
        "    log[\"model_fit_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    print(time.time()-st)\n",
        "    predictions=model.transform(test_data)\n",
        "    log[\"prediction_time\"]=time.time()-st\n",
        "    st=time.time()\n",
        "    # Select (prediction, true label) and compute test error\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"score\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "    print(\"Test set accuracy = \" + str(accuracy))\n",
        "    print(time.time()-st)\n",
        "    log[\"evaulation_time\"]=time.time()-st\n",
        "    log[\"full_time\"]=time.time()-fulltime\n",
        "    logs.append(log)\n",
        "    print(logs)\n",
        "    print(time.time()-st)\n",
        "    with open(\"logs.csv\", mode='a', newline='') as file:\n",
        "        # Create a CSV writer object\n",
        "        csv_writer = csv.writer(file)\n",
        "\n",
        "        # If the file is empty, write the header\n",
        "        if file.tell() == 0:\n",
        "            csv_writer.writerow(list(log.keys()))\n",
        "        # Write the new data to the end of the CSV file\n",
        "        csv_writer.writerow(list(log.values()))\n",
        "\n",
        "    del df\n",
        "    del train_data\n",
        "    del test_data\n",
        "    del scaled_df\n",
        "    del final_data\n",
        "    del columns\n",
        "    del incols\n",
        "    del log\n",
        "    del model\n",
        "    del logs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}